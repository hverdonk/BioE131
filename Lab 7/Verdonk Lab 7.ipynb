{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each file has 100Mb of randomly generated binary data. Six files were generated, with fifty-, sixty-, seventy-, eighty-, ninety-, and one hundred percent zeros, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fiftypercent0 = np.random.choice([0, 1], size=(8, 1024, 1024, 100), replace=True, p=[0.5, 0.5])\n",
    "zeros_50p = np.packbits(fiftypercent0)\n",
    "open('zeros_50p', 'wb+').write(zeros_50p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sixtypercent0 = np.random.choice([0, 1], size=(8, 1024, 1024, 100), replace=True, p=[0.6, 0.4])\n",
    "zeros_60p = np.packbits(sixtypercent0)\n",
    "open('zeros_60p', 'wb+').write(zeros_60p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104857600"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "seventypercent0 = np.random.choice([0, 1], size=(8, 1024, 1024, 100), replace=True, p=[0.7, 0.3])\n",
    "zeros_70p = np.packbits(seventypercent0)\n",
    "open('zeros_70p', 'wb+').write(zeros_70p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104857600"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "eightypercent0 = np.random.choice([0, 1], size=(8, 1024, 1024, 100), replace=True, p=[0.8, 0.2])\n",
    "zeros_80p = np.packbits(eightypercent0)\n",
    "open('zeros_80p', 'wb+').write(zeros_80p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104857600"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ninetypercent0 = np.random.choice([0, 1], size=(8, 1024, 1024, 100), replace=True, p=[0.9, 0.1])\n",
    "zeros_90p = np.packbits(ninetypercent0)\n",
    "open('zeros_90p', 'wb+').write(zeros_90p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104857600"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "hundredpercent0 = np.random.choice([0, 1], size=(8, 1024, 1024, 100), replace=True, p=[1.0, 0.0])\n",
    "zeros_100p = np.packbits(hundredpercent0)\n",
    "open('zeros_100p', 'wb+').write(zeros_100p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are random nucleotide and protein sequences, respectively. Each was 100 million letters long, with equal probability of puling any letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate random DNA sequence\n",
    "import numpy as np\n",
    "\n",
    "my_nt_seq = np.random.choice(['A', 'T', 'G', 'C'], size=(100000000), replace=True, p=[0.25, 0.25, 0.25, 0.25])\n",
    "open('nt_seq.fa', 'w').write(''.join(my_nt_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate random protein sequence\n",
    "import numpy as np\n",
    "\n",
    "amino_acids = ['R', 'H', 'K', 'D', 'E', 'S', 'T', 'N', 'Q', 'C', 'G', 'P', 'A', 'V', 'I', 'L', 'M', 'F', 'Y', 'W']\n",
    "\n",
    "my_prot_seq = np.random.choice(amino_acids, size=(100000000), replace=True)\n",
    "open('prot_seq.fa', 'w').write(''.join(my_prot_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the terminal window, we ran the following commands on each file:  \n",
    "\n",
    "```\n",
    "time gzip -k [filename]\n",
    "time bzip2 -k [filename]\n",
    "time pbzip2 -k [filename]\n",
    "time ArithmeticCompress [filename] [filename].art\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nt_seq.fa  \n",
    "\n",
    "input size: 100 MB  \n",
    "\n",
    "**gzip**\n",
    "```\n",
    "real    0m12.193s\n",
    "user    0m12.120s\n",
    "sys     0m0.073s\n",
    "```\n",
    "output size: 29.2 MB  \n",
    "\n",
    "**bzip2**\n",
    "```\n",
    "real    0m9.483s\n",
    "user    0m9.434s\n",
    "sys     0m0.049s\n",
    "```\n",
    "output size: 27.3 MB  \n",
    "\n",
    "**pbzip2**\n",
    "```\n",
    "real    0m0.663s\n",
    "user    0m15.422s\n",
    "sys     0m0.545s\n",
    "```\n",
    "output size: 27.3 MB  \n",
    "\n",
    "**ArithmeticCompress**\n",
    "```\n",
    "real    0m21.533s\n",
    "user    0m21.376s\n",
    "sys     0m0.157s\n",
    "```\n",
    "output size: 25 MB  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prot_seq.fa\n",
    "\n",
    "input size: 100 MB  \n",
    "\n",
    "**gzip**\n",
    "```\n",
    "real    0m4.234s\n",
    "user    0m4.149s\n",
    "sys     0m0.084s\n",
    "```\n",
    "output size: 60.6 MB  \n",
    "\n",
    "**bzip2**\n",
    "```\n",
    "real    0m10.761s  \n",
    "user    0m10.648s  \n",
    "sys     0m0.112s\n",
    "```\n",
    "output size: 55.3 MB  \n",
    "\n",
    "**pbzip2**\n",
    "```\n",
    "real    0m0.799s\n",
    "user    0m18.397s\n",
    "sys     0m0.580s\n",
    "```\n",
    "output size: 55.3 MB  \n",
    "\n",
    "**ArithmeticCompress**\n",
    "```\n",
    "real    0m28.763s\n",
    "user    0m28.654s\n",
    "sys     0m0.104s\n",
    "```\n",
    "output size: 54 MB  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zeros_50p\n",
    "\n",
    "input size: 105 MB  \n",
    "\n",
    "**gzip**\n",
    "```\n",
    "real    0m3.588s\n",
    "user    0m3.450s\n",
    "sys     0m0.136s\n",
    "```\n",
    "output size: 105 MB  \n",
    "\n",
    "**bzip2**\n",
    "```\n",
    "real    0m16.761s\n",
    "user    0m16.624s\n",
    "sys     0m0.136s\n",
    "```\n",
    "output size: 105 MB  \n",
    "\n",
    "**pbzip2**\n",
    "```\n",
    "real    0m1.548s\n",
    "user    0m40.266s\n",
    "sys     0m0.926s\n",
    "```\n",
    "output size: 105 MB  \n",
    "\n",
    "**ArithmeticCompress**\n",
    "```\n",
    "real    0m40.835s\n",
    "user    0m40.619s\n",
    "sys     0m0.212s\n",
    "```\n",
    "output size: 105 MB  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zeros_60p\n",
    "\n",
    "input size: 105 MB  \n",
    "\n",
    "**gzip**\n",
    "```\n",
    "real    0m4.455s\n",
    "user    0m4.131s\n",
    "sys     0m0.169s\n",
    "```\n",
    "output size: 102 MB  \n",
    "\n",
    "**bzip2**\n",
    "```\n",
    "real    0m15.889s\n",
    "user    0m15.765s\n",
    "sys     0m0.124s\n",
    "```\n",
    "output size: 105 MB  \n",
    "\n",
    "**pbzip2**\n",
    "```\n",
    "real    0m1.393s\n",
    "user    0m36.610s\n",
    "sys     0m0.899s\n",
    "```\n",
    "output size: 105 MB  \n",
    "\n",
    "**ArithmeticCompress**\n",
    "```\n",
    "real    0m41.776s\n",
    "user    0m41.499s\n",
    "sys     0m0.228s\n",
    "```\n",
    "output size: 102 MB  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zeros_70p\n",
    "\n",
    "input size: 105 MB\n",
    "\n",
    "**gzip**\n",
    "```\n",
    "real    0m6.012s\n",
    "user    0m5.887s\n",
    "sys     0m0.120s\n",
    "```\n",
    "output size: 93.6 MB  \n",
    "\n",
    "**bzip2**\n",
    "```\n",
    "real    0m14.361s\n",
    "user    0m14.220s\n",
    "sys     0m0.140s\n",
    "```\n",
    "output size: 99.8 MB  \n",
    "\n",
    "**pbzip2**\n",
    "```\n",
    "real    0m1.098s\n",
    "user    0m28.479s\n",
    "sys     0m0.726s\n",
    "```\n",
    "output size: 99.8 MB  \n",
    "\n",
    "**ArithmeticCompress**\n",
    "```\n",
    "real    0m39.947s\n",
    "user    0m39.654s\n",
    "sys     0m0.292s\n",
    "```\n",
    "output size: 92.4 MB  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zeros_80p\n",
    "\n",
    "input size: 105 MB  \n",
    "\n",
    "**gzip**\n",
    "```\n",
    "real    0m13.549s\n",
    "user    0m13.260s\n",
    "sys     0m0.120s\n",
    "```\n",
    "output size: 81.2 MB  \n",
    "\n",
    "**bzip2**\n",
    "```\n",
    "real    0m12.053s\n",
    "user    0m11.961s\n",
    "sys     0m0.092s\n",
    "```\n",
    "output size: 86.6 MB  \n",
    "\n",
    "**pbzip2**\n",
    "```\n",
    "real    0m0.943s\n",
    "user    0m23.715s\n",
    "sys     0m0.901s\n",
    "```\n",
    "output size: 86.7 MB  \n",
    "\n",
    "**ArithmeticCompress**\n",
    "```\n",
    "real    0m35.859s\n",
    "user    0m35.686s\n",
    "sys     0m0.172s\n",
    "```\n",
    "output size: 75.7 MB  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zeros_90p  \n",
    "\n",
    "input size: 105 MB  \n",
    "\n",
    "**gzip**\n",
    "```\n",
    "real    0m19.056s\n",
    "user    0m18.894s\n",
    "sys     0m0.108s\n",
    "```\n",
    "output size: 58.7 MB  \n",
    "\n",
    "**bzip2**\n",
    "```\n",
    "real    0m10.726s\n",
    "user    0m10.674s\n",
    "sys     0m0.052s\n",
    "```\n",
    "output size: 61.2 MB  \n",
    "\n",
    "**pbzip2**\n",
    "```\n",
    "real    0m0.767s\n",
    "user    0m19.069s\n",
    "sys     0m0.752s\n",
    "```\n",
    "output size: 61.2 MB  \n",
    "\n",
    "**ArithmeticCompress**\n",
    "```\n",
    "real    0m29.088s\n",
    "user    0m28.792s\n",
    "sys     0m0.296s\n",
    "```\n",
    "output size: 49.2 MB  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zeros_100p  \n",
    "\n",
    "input size: 105 MB  \n",
    "\n",
    "**gzip**\n",
    "```\n",
    "real    0m0.723s\n",
    "user    0m0.682s\n",
    "sys     0m0.036s\n",
    "```\n",
    "output size: 102 kB  \n",
    "\n",
    "**bzip2**\n",
    "```\n",
    "real    0m1.030s\n",
    "user    0m0.985s\n",
    "sys     0m0.045s\n",
    "```\n",
    "output size: 113 B  \n",
    "\n",
    "**pbzip2**\n",
    "```\n",
    "real    0m0.112s\n",
    "user    0m1.985s\n",
    "sys     0m0.110s\n",
    "```\n",
    "output size: 5.62 kB  \n",
    "\n",
    "**ArithmeticCompress**\n",
    "```\n",
    "real    0m14.908s\n",
    "user    0m14.883s\n",
    "sys     0m0.025s\n",
    "```\n",
    "output size: 1.03 kB  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Questions\n",
    "\n",
    "**Which algorithm achieves the best level of compression on each file type?**  \n",
    "Overall, the ArithmeticCompress algorithm achieved the best level of compression.  \n",
    "\n",
    "**Which algorithm is the fastest?**  \n",
    "Based on the \"real\" time output by the terminal, pbzip2 was the fastest algorithm for all files.  \n",
    "\n",
    "**What is the difference between bzip2 and pbzip2? Do you expect one to be faster and why?**  \n",
    "The pbzip2 algorithm supports multi-threading, but the bzip2 algorithm does not. Because of this, it is likely that pbzip2 is faster.   \n",
    "\n",
    "**How does the level of compression change as the percentage of zeros increases? Why does this happen?**  \n",
    "At 50% zeros, the ideal code for encoding the file is binary. Because the file is already in binary, no file compression algorithm can substitute that for another code that will compress it further (and most - if not all - will actually increase the size of the file slightly to store metadata, or will represent the digits as 8-bit ASCII characters rather than single bits). However, as the percentage of zeros increases, there will be more runs of similar characters, and therefore the compression algorithms we tested can actually *compress* those runs down.  \n",
    "\n",
    "**What is the minimum number of bits required to store a single DNA base?**  \n",
    "There are 4 DNA bases. To uniquely identify one, we need log2(4 bases)= 2 bits minimum.  \n",
    "\n",
    "**What is the minimum number of bits required to store an amino acid letter?**  \n",
    "There are 20 amino acids. To uniquely identify one, we need log2(20 aa)= 4.3219 bits. Because we cannot use fractions of a bit, we must go up to 5 bits as the minimum number required to store a single amino acid letter.  \n",
    "\n",
    "**In your tests, how many bits did gzip and bzip2 actually require to store your random DNA and protein sequences?**  \n",
    "\n",
    ">**_Random DNA Sequence:_**   \n",
    "*gzip:* 29.2 MB  \n",
    "*bzip2:* 27.3 MB   \n",
    "\n",
    ">**_Random Protein Sequence:_**   \n",
    "*gzip:* 60.6 MB  \n",
    "*bzip2:* 55.3 MB  \n",
    "  \n",
    "**Are gzip and bzip2 performing well on DNA and proteins?**  \n",
    "Given that they both reduced 100 MB files to around half their original size or less, I'd say that they are performing quite well in terms of compression.\n",
    "\n",
    "In terms of runtime, both took (at worst) 10-12 seconds to run in real time, so they are performing decently there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 20 results of an Entrez nucleotide database query for \"Human immunodeficiency virus 1[ORGN] gp120\" were parsed, concatenated, and stored in a single FASTA file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from Bio import SeqIO\n",
    "Entrez.email = 'hverdonk@berkeley.edu'\n",
    "\n",
    "#create the FASTA file\n",
    "fasta = open(\"HIV_gp120_isolates.fatsa\", \"w+\")\n",
    "\n",
    "#search for entries about different HIV gp120 gene isolates\n",
    "handle = Entrez.esearch(db='nucleotide',\n",
    "                        term=\"Human immunodeficiency virus 1[ORGN] gp120\",\n",
    "                        sort='relevance',\n",
    "                        idtype='acc')\n",
    "results = Entrez.read(handle)['IdList']\n",
    "\n",
    "#for the first 20 results, add them to a FASTA file\n",
    "for i in range(20):\n",
    "    handle=Entrez.efetch(db='nucleotide', id=results[i], rettype='gb', retmode='text')\n",
    "    entry = SeqIO.read(handle, 'gb')\n",
    "    \n",
    "    name = entry.name\n",
    "    seq = str(entry.seq)\n",
    "    fasta.write(\">\" + name + \"\\n\" + seq + \"\\n\")\n",
    "fasta.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A priori, do you expect to achieve better or worse compression here than random data? Why?**  \n",
    "HIV_gp120_isolates.fatsa will likely have better compression than random data since real genes have more repetition of particular nucleotides, which is beneficial to compression algorithms.  \n",
    "\n",
    "**How does the compression ratio of this file compare to random data?**  \n",
    "For the random nucleotide data, the compression ratios were as follows:  \n",
    ">*gzip:* 29.2 MB/100 MB = 0.292  \n",
    "*bzip2:* 27.3 MB/100 MB = 0.273  \n",
    "*ArithmeticCompression:* 25 MB/100 MB = 0.25  \n",
    "\n",
    "For HIV_gp120_isolates.fasta, the compression ratios were as follows:  \n",
    ">*gzip:* 1.52 kB/7.26 kB = 0.209  \n",
    "*bzip2:* 1.57 kB/7.26 kB = 0.216  \n",
    "*ArithmeticCompression:* 2.94 kB/7.26 kB = 0.404  \n",
    "\n",
    "When run on HIV_gp120_isolates.fasta, gzip and bzip2 slightly outperformed themselves compared to when they were run on random data. However, ArithmeticCompression did significantly better on the random data than on HIV_gp120_isolates.fasta.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Compression of 1,000 Terabytes\n",
    "\n",
    "1 TB = 1,000,000 MB\n",
    "1,000 TB = 1,000,000,000 MB\n",
    "We'll assume that compression time scales linearly with file size. Days are measured in 24 hours (rather than an 8 hour work day).\n",
    "\n",
    "### \"Most of the data, say 80%, is re-sequencing of genomes and plasmids that are very similar to each other\"  \n",
    "That's 800 TB, or about 800,000,000 MB (8,000,000 times greater than our 100 MB nucleotide sequence test dataset). The data are as follows:  \n",
    "\n",
    ">*input size:* 100 MB  \n",
    "*gzip:* 29.2 MB (12.193s)  \n",
    "*bzip2:* 27.3 MB (9.483s)  \n",
    "*pbzip2:* 27.3 MB (0.663s)  \n",
    "*ArithmeticCompression:* 25 MB (21.533s)  \n",
    "\n",
    "To compress 1,000 TB, it would take\n",
    "\n",
    "**gzip**\n",
    ">*memory:* 29.2 MB * 8,000,000 = 233.6 TB  \n",
    "*time:* 12.193s * 8,000,000 = 1,128.981 days   \n",
    "*amount of data that could be compressed in a day:* 233.6 TB / 1,128.981 days = 0.206 TB/day  \n",
    "\n",
    "**bzip2** \n",
    ">*memory:* 27.3 MB * 8,000,000 = 218.4 TB  \n",
    "*time:* 9.483s * 8,000,000 = 878.055 days  \n",
    "*amount of data that could be compressed in a day:* 218.4 TB / 878.055 days = 0.248 TB/day  \n",
    "\n",
    "**pbzip2** \n",
    ">*memory:* 27.3 MB * 8,000,000 = 218.4 TB  \n",
    "*time:* 0.663s * 8,000,000 = 61.388 days  \n",
    "*amount of data that could be compressed in a day:* 218.4 TB / 61.388 days = 3.552 TB/day  \n",
    "\n",
    "**ArithmeticCompression** \n",
    ">*memory:* 25 MB * 8,000,000 = 200 TB  \n",
    "*time:* 21.533s * 8,000,000 = 1,993.796 days  \n",
    "*amount of data that could be compressed in a day:* 200 TB / 1,993.796 days = 0.100 TB/day  \n",
    "\n",
    "\n",
    "### \"Another 10% might be protein sequences\" \n",
    "That's 100 TB, or about 100,000,000 MB (1,000,000 times greater than our 100 MB protein sequence test dataset). The data are as follows:  \n",
    "\n",
    ">*input size:* 100 MB  \n",
    "*gzip:* 60.6 MB (4.234s)  \n",
    "*bzip2:* 55.3 MB (10.761s)  \n",
    "*pbzip2:* 55.3 MB (0.799s)  \n",
    "*ArithmeticCompress:* 54 MB (28.763s)  \n",
    "\n",
    "To compress 1,000 TB, it would take\n",
    "\n",
    "**gzip**\n",
    ">*memory:* 60.6 MB * 1,000,000 = 606 TB  \n",
    "*time:* 4.234s * 1,000,000 = 49.004 days  \n",
    "*amount of data that could be compressed in a day:* 606 TB / 49.004 days = 12.366 TB/day  \n",
    "\n",
    "**bzip2** \n",
    ">*memory:* 55.3 MB * 1,000,000 = 553 TB  \n",
    "*time:* 10.761s * 1,000,000 = 124.548 days  \n",
    "*amount of data that could be compressed in a day:* 553 TB / 124.548 days = 4.44 TB/day  \n",
    "\n",
    "**pbzip2** \n",
    ">*memory:* 55.3 MB * 1,000,000 = 553 TB  \n",
    "*time:* 0.799s * 1,000,000 = 9.247 days  \n",
    "*amount of data that could be compressed in a day:* 553 TB / 9.247 days = 59.798 TB/day  \n",
    "\n",
    "**ArithmeticCompression** \n",
    ">*memory:* 54 MB * 1,000,000 = 540 TB  \n",
    "*time:* 28.763s * 1,000,000 = 332.905 days  \n",
    "*amount of data that could be compressed in a day:* 540 TB / 332.905 days = 1.622 TB/day  \n",
    " \n",
    "\n",
    "### The last 10% are binary microscope images which we’ll assume follow the worst-case scenario of being completely random.\n",
    "That's another 100 TB, or about 100,000,000 MB (952,380.952 times greater than our 105 MB random binary test dataset). Assuming these binary files are roughly 50% zeros (given that they are completely random), the data are as follows:  \n",
    "\n",
    ">*input size:* 105 MB  \n",
    "*gzip:* 105 MB (3.588s)  \n",
    "*bzip2:* 105 MB (16.761s)  \n",
    "*pbzip2:* 105 MB (1.548s)  \n",
    "*ArithmeticCompress:* 105 MB (40.835s)  \n",
    "\n",
    "To compress 1,000 TB, it would take\n",
    "\n",
    "**gzip**\n",
    ">*memory:* 105 MB * 952,380.952 = 99.999 TB  \n",
    "*time:* 3.588s * 952,380.952 = 39.550 days  \n",
    "*amount of data that could be compressed in a day:* 99.999 TB / 39.550 days = 2.528 TB/day  \n",
    "\n",
    "**bzip2** \n",
    ">*memory:* 105 MB * 952,380.952 = 99.999 TB  \n",
    "*time:* 16.761s * 952,380.952 = 184.755 days  \n",
    "*amount of data that could be compressed in a day:* 99.999 TB / 184.755 days = 0.541 TB/day  \n",
    "\n",
    "**pbzip2** \n",
    ">*memory:* 105 MB * 952,380.952 = 99.999 TB  \n",
    "*time:* 1.548s * 952,380.952 = 17.063 days  \n",
    "*amount of data that could be compressed in a day:* 99.999 TB / 17.063 days = 5.860 TB/day  \n",
    "\n",
    "**ArithmeticCompression** \n",
    ">*memory:* 105 MB * 952,380.952 = 99.999 TB  \n",
    "*time:* 40.835s * 952,380.952 = 450.121 days  \n",
    "*amount of data that could be compressed in a day:* 99.999 TB / 450.121 days = 0.222 TB/day  \n",
    "\n",
    "\n",
    "### Given the benchmarking data you obtained in this lab, which algorithm do you propose to use for each type of data? Provide an estimate for the fraction of space you can save using your compression scheme. How much of a bonus do you anticipate receiving this year?\n",
    "\n",
    "For each data type, pbzip2 consistently compressed the largest fraction of the data per day. \n",
    "\n",
    "**Space saved per year using pbzip2:** 365*(3.552 TB genome sequences/day + 59.798 TB protein sequences/day + 5.860 TB binary image data/day) = 25,261.65 TB\n",
    "\n",
    "**Expected Bonus:** 25,261.65 TB * 50 dollars per TB = 1,263,082.5 dollars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
